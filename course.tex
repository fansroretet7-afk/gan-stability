\documentclass[12pt]{article}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{geometry}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{float}
\usepackage[russian]{babel}
\geometry{margin=1in}
\usepackage{setspace}
\onehalfspacing


\title{Сходимость и устойчивость обучения GAN как динамической игры}
\author{Самвелян Артур Хачатурович}
\date{2025 г.}

\begin{document}
\maketitle

	\section{Введение}
	
	Генеративно-состязательные сети (GAN) представляют собой динамическую игру между генератором $G$ и дискриминатором $D$. 
	Вместо простой оптимизации функция потерь разделена на две противоположно направленные части, что приводит к сложным нелинейным динамическим эффектам: вращениям, циклам, неустойчивости, расходимости.
	
	Цель данной работы - разобрать теоретические основы локальной устойчивости обучения GAN, показать влияние разложения Якобиана на симметричную и антисимметричную части, доказать свойства игрушечной билинейной игры, а также провести эксперименты на 2D-датасете и MNIST.
	
	\subsection{Терминология}
	
	\textbf{Градиентный поток} - непрерывная динамика параметров:
	\[
	\dot{\theta} = -v(\theta)
	\]
	
	\textbf{Якобиан} - матрица частных производных градиентного поля.
	
	\textbf{Симметричная часть $S$} - отвечает за устойчивость:
	\[
	S = \frac{J + J^\top}{2}
	\]
	
	\textbf{Антисимметричная часть $A$} - порождает вращение:
	\[
	A = \frac{J - J^\top}{2}
	\]
	
	\textbf{Собственные значения} - определяют локальную динамику.
	
	\textbf{Extragradient, Optimistic GD} - методы стабилизации игр.
	
	\textbf{R1-регуляризация} - штраф на градиент $D$ по реальным данным.
	
	\section{Краткий конспект}
	\title{Обучение GAN как динамика в параметрах}
	Обучение GAN можно рассматривать как динамическую систему в пространстве параметров генератора и дискриминатора.
	\begin{itemize}
		\item \textbf{Дискретная форма}:
		
		\begin{equation}
			\theta_G^{t+1} = \theta_G^t - \eta_G \nabla_{\theta_G} L_G(\theta_G^t, \theta_D^t), \quad
			\theta_D^{t+1} = \theta_D^t - \eta_D \nabla_{\theta_D} L_D(\theta_G^t, \theta_D^t),
		\end{equation}
		где $L_G$ и $L_D$ --- функции потерь генератора и дискриминатора, $\eta_G, \eta_D$ --- скорости обучения.
		
		\item \textbf{Градиентное поле}:
		В пределе $\eta \to 0$ динамику можно рассматривать как непрерывное градиентное поле:
		\begin{equation}
			\dot{\theta} = v(\theta), \quad
			v(\theta) =
			\begin{pmatrix}
				\nabla_{\theta_G} L_G(\theta_G, \theta_D) \\
				-\nabla_{\theta_D} L_D(\theta_G, \theta_D)
			\end{pmatrix}.
		\end{equation}
		Знак минус перед градиентом дискриминатора отражает его задачу максимизации.
	\end{itemize}
	

	
	
	
	\subsection{Локальная устойчивость }
	
	Пусть $(\theta_G^*, \theta_D^*)$ --- стационарная точка . Рассмотрим линейную аппроксимацию динамики около этой точки:
	\begin{equation}
		\dot{\theta} \approx v(\theta^*) + J(\theta^*) (\theta - \theta^*),
	\end{equation}
	где $J(\theta^*) = \nabla_\theta v(\theta^*)$ --- якобиан в точке равновесия.
	
	Так как $v(\theta^*) = 0$, линейная система имеет вид:
	\begin{equation}
		\dot{\theta} = J(\theta^*)(\theta - \theta^*) \quad \text{или} \quad
		\dot{\theta} = -J(\theta^*)(\theta - \theta^*),
	\end{equation}
	
	\subsection{Роль симметричной и антисимметричной частей якобиана}
	
	Разделим якобиан на симметричную и антисимметричную части:
	\begin{equation}
		J = S + A, \quad S = \frac{J + J^\top}{2}, \quad A = \frac{J - J^\top}{2}.
	\end{equation}
	
	\begin{itemize}
		\item Симметричная часть $S$ отвечает за затухание/развитие модулей колебаний (экспоненциальную сходимость/расходимость).
		\item Антисимметричная часть $A$ отвечает за вращение траекторий (спиральные или циклические движения вокруг равновесия).
	\end{itemize}
	
	Типичные GAN-динамики имеют значительную антисимметричную часть, что приводит к вращательным движениям и затрудняет прямую сходимость.
	
	\subsection{Регуляризации и методы стабилизации}
	
	Исследования показывают, что локальная сходимость GAN можно улучшить с помощью регуляризаций, схем шагов и модифицированных оптимизаторов.
	
	\subsubsection{Регуляризации}
	
	\begin{itemize}
		\item \textbf{R1 / R2 regularization}: добавление штрафа на градиент дискриминатора
		\begin{equation}
			\text{R1: } \lambda \|\nabla_x D(x)\|^2 \quad \text{(реальные данные)}, \quad
			\text{R2: } \lambda \|\nabla_x D(G(z))\|^2 \quad \text{(сгенерированные данные)}.
		\end{equation}
		Уменьшает антисимметричные компоненты якобиана.
		
		\item \textbf{Instance noise}: добавление шума к реальным и сгенерированным данным, сглаживает потоки градиента и улучшает локальную устойчивость.
	\end{itemize}
	
	\subsubsection{Схемы шагов (learning rates)}
	
	\textbf{TTUR (Two Time-Scale Update Rule)}: разные скорости обучения для генератора и дискриминатора ($\eta_G \neq \eta_D$).  
	Позволяет уменьшить цикличность и стабилизировать динамику.
	
	\subsubsection{Оптимизаторы и методы}
	
	\begin{itemize}
		\item \textbf{Экстраградиент (Extragradient)}: делается предварительный градиентный шаг, затем обновление с учетом него. Эффективно устраняет вращательные компоненты.
		\item \textbf{Optimistic gradient / Optimistic Mirror Descent}: использует предсказания будущих градиентов, снижает цикличность и ускоряет затухание в седловых точках.
	\end{itemize}
	
	\subsection{Выводы}
	
	\begin{itemize}
		\item Стандартный градиентный спуск в GAN часто неустойчив из-за антисимметричной части якобиана.
		\item R1/R2, instance noise и TTUR помогают стабилизировать локальное поведение.
		\item Экстраградиентные методы и оптимистичные обновления эффективно борются с вращательными компонентами, делая локальную сходимость более предсказуемой.
		\item Линейная аппроксимация и разложение якобиана на симметричную/антисимметричную части дают чёткую интерпретацию динамики и выбор инструментов стабилизации.
	\end{itemize}
	
	
	
	\section{Мини-теоретический результат}
	
	Рассмотрим мини-игру (билинейный случай)
	\[
	f(x,y)=x^\top A y,\qquad x\in\mathbb{R}^n,\; y\in\mathbb{R}^m,
	\]
	и стандартный режим ``градиент-спуск по \(x\) / градиент-восход по \(y\)'' (GD/GA). 
	Обозначим \(z=\begin{bmatrix}x\\ y\end{bmatrix}\in\mathbb{R}^{n+m}\).
	
	\begin{proof}[GD/GA даёт чистую ротацию (мнимые собственные значения)]
		Для непрерывного времени (gradient flow)
		\[
		\dot x = -\nabla_x f(x,y) = -A y,\qquad
		\dot y = +\nabla_y f(x,y) = A^\top x,
		\]
		стекнутая система записывается как
		\[
		\dot z = S z,\qquad 
		S=\begin{bmatrix}0 & -A\\[2pt] A^\top & 0\end{bmatrix}.
		\]
		Матрица \(S\) является кососимметрической: \(S^\top = -S\). Отсюда все собственные значения \(S\) лежат на чисто мнимой оси (если \(\lambda\) — собственное значение, то \(\bar\lambda=-\lambda\)), и динамика линейно представляет собой чистую ротацию (нет экспоненциального сжатия/расширения).
	\end{proof}
	\begin{proof}
		Прямая проверка: \(S^\top = \begin{bmatrix}0 & A\\ -A^\top & 0\end{bmatrix} = -S\).
		Для любой вещественной кососимметрической матрицы собственные значения либо нулевые, либо чисто мнимые в паре \(\pm i\omega\). Поэтому линейный поток сохраняет норму (ротация/сдвиг) и не имеет отрицательной части — нет локальной экспоненциальной устойчивости (только нейтральная).
	\end{proof}
	
	Таким образом, в непрерывной модели GD/GA даёт ортогональную (ротационную) часть без демпфирования.
	
	\bigskip
	
	Теперь рассмотрим явный \emph{экстраградиент} (Korpelevich) в дискретном времени для билинейного случая.
	Одноступенчатый EG для шага \(\eta>0\):
	\[
	\begin{aligned}
		x^{1/2} &= x_t - \eta A y_t,\\
		y^{1/2} &= y_t + \eta A^\top x_t,\\
		x_{t+1} &= x_t - \eta A y^{1/2},\\
		y_{t+1} &= y_t + \eta A^\top x^{1/2}.
	\end{aligned}
	\]
	Подставляя полу-шаги, получаем линейный одношаговый оператор
	\[
	z_{t+1} = M_{\text{EG}}\, z_t,
	\]
	где (неявно) можно записать
	\begin{equation}\label{eq:EG-linear}
		z_{t+1} = \big(I + \eta S - \eta^2 K\big) z_t,
		\qquad
		K=\begin{bmatrix}AA^\top & 0\\[2pt] 0 & A^\top A\end{bmatrix}.
	\end{equation}
	Здесь \(S\) — та же кососимметрическая матрица, а \(K\) — симметрическая неотрицательно определённая.
	
	\begin{proof}[Экстраградиент добавляет симметричную (демпфирующую) часть]
		Матрица \(M_{\text{EG}}=I+\eta S-\eta^2 K\) имеет симметричную часть
		\[
		\frac{M_{\text{EG}}+M_{\text{EG}}^\top}{2}=I-\eta^2 K,
		\]
		поскольку \(S^\top=-S\) и \(K^\top=K\). Поскольку \(K\succeq0\), при ненулевом \(\eta\) терм \(-\eta^2 K\) вносит отрицательное \emph{семи}определённое пополнение к симметричной части (в дискретном виде — уменьшает собственные значения симметричной части матрицы итерации); эквивалентно, в эквивалентной непрерывной аппроксимации это даёт демпфирование (положительную симметричную часть в операторе, отвечающую за обратное сжатие).
		В частности, для достаточно малого шага \(\eta\) спектральный радиус \( \rho(M_{\text{EG}})\) может быть меньше единицы, т.е. итерация становится локально сходящейся.
	\end{proof}
	
	\paragraph{Вывод.} Для билинейной игры GD/GA (в непрерывном лимите) даёт чистую ротацию (нет демпфирования). Экстраградиент (или оптимистичные схемы) приводят к появлению симметричной (положительной/сжимающей в непрерывном эквиваленте) компоненты, которая уменьшает вращение и делает динамику локально устойчивой при подходящем шаге.
	
	\bigskip
	
	\section*{Скалярный случай GAN-потерь: эскиз для \(R_1\)}
	
	Рассмотрим простую модель: дискриминатор с однопараметрическим выходом \(D_\theta(x)\) (параметр \(\theta\in\mathbb{R}\)), генератор с параметром \(\phi\in\mathbb{R}\). Пусть цель в одноточечном упрощении задаётся функцией
	\[
	L(\theta,\phi)=\mathbb{E}_{x\sim p_{\text{data}}}\ell\big(D_\theta(x)\big)
	- \mathbb{E}_{z\sim p_z}\ell\big(D_\theta(G_\phi(z))\big),
	\]
	и рассмотрим игру \(\min_\phi \max_\theta L(\theta,\phi)\). В простейшем локальном анализе вокруг равновесия можно линераизовать зависимости параметров генератора через отклик на \(\phi\) (линейно): пусть эффективно вклад генератора в градиент дискриминатора равен \(-b\phi\) (то есть \(\partial_\theta L\approx -b\phi\)), а вклад дискриминатора в градиент генератора равен \(a\theta\) (т.е. \(\partial_\phi L\approx a\theta\)). Тогда в линейном приближении векторное поле (градиент-спуск по \(\phi\), градиент-восход по \(\theta\)) имеет вид
	\[
	v(\theta,\phi)=\begin{bmatrix}-\partial_\theta L\\[2pt]\partial_\phi L\end{bmatrix}
	\approx
	\begin{bmatrix} b\phi \\[2pt] a\theta \end{bmatrix}
	=
	\underbrace{\begin{bmatrix}0 & b\\ a & 0\end{bmatrix}}_{J_0}
	\begin{bmatrix}\theta\\ \phi\end{bmatrix}.
	\]
	Матрица \(J_0\) имеет нулевые диагонали и антисимметрическую часть, которая отвечает за вращение: действительно при \(a=-b\) это чисто косая матрица. В общем случае \(J_0\) может быть разложена на симметричную и косую части; но основная причинно-следственная механика вращения — наличие перекрёстных (взаимных) производных \(\partial_{\theta\phi}^2 L\).
	
	Теперь добавим штраф \(R_1=\tfrac{\lambda}{2}\,\mathbb{E}_{x\sim p_{\text{data}}}\|\nabla_x D_\theta(x)\|^2\). Этот штраф влияет только на обновление дискриминатора (прибавляет вклад в градиент по \(\theta\)), и при линейной аппроксимации вокруг точки данных даёт дополнительный член во второй производной по \(\theta\):
	\[
	\partial_\theta L \;\mapsto\; \partial_\theta L + \lambda\,H_\theta,
	\]
	где \(H_\theta\) — некоторая (положительно полуопределённая) величина, равная сумме (по реальным данным) квадратов производных модели по параметрам (локально — матричный квадратичный вклад). В трактовке якобиана это означает:
	\[
	J \;=\;
	\begin{bmatrix} \;0\;&\; b\;\\[2pt] a\;&\;0\;\end{bmatrix}
	\quad\mapsto\quad
	J_{\text{R1}} \;=\;
	\begin{bmatrix} \;-\lambda h\;&\; b\;\\[2pt] a\;&\;0\;\end{bmatrix},
	\]
		где $h \ge 0$ --- локальная (положительная) величина, возникающая из $R_1$.
		Теперь симметричная часть матрицы $J_{\text{R1}}$ содержит элемент $-\tfrac{\lambda h}{2}$
		по диагонали (и, в общем, положительную/негативную поправку в зависимости от знака),
		что эквивалентно появлению сжимающего (диссипативного) члена, уменьшающего
		чисто ротационную компоненту.
		(В более общем многомерном случае $R_1$ даёт вклад в верхний левый блок якобиана
		в виде положительно полуопределённой матрицы, см.\ Mescheder et al.\@: это уменьшает
		антисимметричную/вращательную долю и вводит демпфирование.)
	
	\paragraph{Интуитивный итог.} Штраф \(R_1\) добавляет в якобиан (по параметрам дискриминатора) положительную симметричную матрицу, т.е. вносит диссипацию в направления, ранее участвовавшие в ротации между генератором и дискриминатором. В результате вращательный компонент движения уменьшается и поведение становится менее «цикличным» и более схожим на сжимающее (успокаивающее) — поэтому \(R_1\) стабилизирует обучение.
	
	\bigskip
	
	\section{Репликация базовых экспериментов}
	
	\subsection{Постановка}
	
	Мы рассматриваем игрушечную постановку обучения GAN на двумерной смеси Гауссианов.
	Генератор $G_\phi$ и дискриминатор $D_\theta$ являются малыми MLP.
	Цель эксперимента — сравнить динамику оптимизации для следующих методов:
	
	\begin{itemize}
		\item \textbf{Vanilla} (одновременный градиентный шаг для $G$ и $D$);
		\item \textbf{R1}-регуляризация ($\|\nabla_x D(x)\|^2$ на реальных данных);
		\item \textbf{Instance noise} (аддитивный гауссов шум к входам дискриминатора);
		\item \textbf{TTUR} (Two Time-Scale Update Rule, разные шаги для $G$ и $D$);
		\item \textbf{Extragradient} (коррекция Korpelevich: половинный шаг → полный шаг);
		\item \textbf{Optimistic} градиент (OGDA).
	\end{itemize}
	
	Для каждого метода мы визуализируем:
	\begin{enumerate}
		\item траектории параметров $(\theta_D, \theta_G)$ в пространстве весов;
		\item спектр Якобиана поля игры $v(\theta)$, где
		\[
		v(\theta) =
		\begin{bmatrix}
			-\nabla_\phi L_G(\theta,\phi) \\
			+\nabla_\theta L_D(\theta,\phi)
		\end{bmatrix},
		\]
		оцениваемый по мини-батчам через конечные разности.
	\end{enumerate}
	
	По спектру фиксируем:
	\[
	\max_i \Re(\lambda_i(J)), \qquad
	\max_i |\Im(\lambda_i(J))|,
	\]
	что позволяет диагностировать \emph{циклы} (крупная мнимая часть)
	и \emph{локальную нестабильность} (положительная вещественная часть).
	
	

	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.95\textwidth]{toy-gan-results.png}
		\caption{
			Траектории параметров $(\theta_G,\theta_D)$ и спектр Якобиана $J$ для разных
			методов оптимизации в 2D toy-GAN.
			\textbf{Vanilla} демонстрирует чистую ротацию и разлёт параметров.
			\textbf{R1} добавляет симметричную часть к $J$ и уменьшает вращение, но при слишком
			большом $\lambda$ возможен численный взрыв.
			\textbf{Instance noise} и \textbf{TTUR} частично уменьшают мнимую часть
			собственных чисел.
			\textbf{Extragradient} и \textbf{Optimistic} формируют выраженную отрицательную
			симметричную часть в $J$, что приводит к демпфированию и устойчивой динамике.
		}
	\end{figure}
	
	
	\subsection{Анализ спектра Якобиана}
	
	Используем стандартное разложение Якобиана:
	\[
	J = S + A,
	\qquad
	S = \tfrac{1}{2}(J + J^\top), 
	\quad A = \tfrac{1}{2}(J - J^\top).
	\]
	
	\begin{itemize}
		\item Кососимметричная часть $A$ отвечает за \emph{чистую ротацию}
		(мнимые собственные значения).
		\item Симметричная часть $S$ определяет \emph{диссипацию} (отрицательная) или
		\emph{разбегание} (положительная).
	\end{itemize}
	
	В экспериментах наблюдаем следующее:
	
	\begin{enumerate}
		\item \textbf{Vanilla}:
		\[
		S \approx 0, \qquad \lambda(J) \approx \pm i\omega,
		\]
		что соответствует отсутствию демпфирования и наличию устойчивой ротации.
		
		\item \textbf{R1}:
		регуляризация добавляет в $S$ отрицательный вклад
		\[
		S \gets S - \lambda H, \qquad H \succeq 0,
		\]
		что уменьшает мнимую часть спектра и может стабилизировать динамику.
		
		\item \textbf{TTUR}:
		изменение масштабов градиентов уменьшает радиус вращения, но полностью
		не устраняет циклическую структуру.
		
		\item \textbf{Extragradient / Optimistic}:
		методы первого порядка, которые \emph{корректируют} векторное поле;
		их действие можно аппроксимировать как
		\[
		J_{\mathrm{EG}} \approx J - \eta J^2,
		\]
		что добавляет отрицательную симметричную часть и создаёт диссипацию.
	\end{enumerate}
	
	
	\textbf{Вывод}
	
	Методы экстраградиентного типа (\textbf{EG}, \textbf{OGDA}) являются наиболее сильными
	в подавлении вращения благодаря появлению отрицательной симметричной части в
	эффективном Якобиане.
	Instance noise и TTUR дают частичное улучшение.
	Vanilla GD/GA остаётся чисто вращательным и нестабильным.
	R1 действует как регуляризатор, который подавляет мнимую составляющую спектра,
	но требует аккуратного выбора гиперпараметров.
	\subsection{MNIST (сильно уменьшенный)}
	
	Для упрощённой постановки эксперимента мы используем сильно уменьшенный датасет MNIST  чтобы ускорить обучение и проверить динамику GAN.  
	Каждое изображение имеет размер $28\times28$ пикселей и нормализуется в диапазон $[-1,1]$.  
	
	\paragraph{Модель.}  
	Используем малые многослойные перцептроны (MLP) для генератора и дискриминатора:
	\begin{itemize}
		\item \textbf{Генератор} $G$: вход — латентный вектор размерности $16$, два слоя с ReLU и Tanh.
		\item \textbf{Дискриминатор} $D$: вход — изображение $28\times28$, два слоя с LeakyReLU и выход через сигмоиду.
	\end{itemize}
	
	\paragraph{Методы сравнения.}  
	Сравниваются следующие подходы:
	\begin{enumerate}
		\item \textbf{Vanilla GD/GA}: обычный градиентный шаг для $G$ и $D$.
		\item \textbf{R1 / R2 регуляризация}: штраф на градиент дискриминатора по реальным / сгенерированным данным.
		\item \textbf{Instance noise}: добавление гауссовского шума к входам дискриминатора.
		\item \textbf{TTUR}: разные скорости обучения для генератора и дискриминатора.
		\item \textbf{Optimistic GD}: корректировка градиентного поля для уменьшения вращательной компоненты.
	\end{enumerate}
	
	\paragraph{Метрики.}  
	Для оценки динамики обучения фиксируются:
	\begin{itemize}
		\item \textbf{Стабильность обучения}: колебания лосса $D$ и $G$ по итерациям.
		\item \textbf{Скорость сходимости}: количество итераций до стабилизации лосса.
		\item \textbf{FID} (по желанию, для оценки качества генерации образцов).
	\end{itemize}
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=1.1\textwidth]{mnist_all_methods.png}
		\caption{
		Сравнение динамики обучения GAN на уменьшенном MNIST.
		Для каждого метода показаны траектории лоссов генератора и дискриминатора.
		Vanilla демонстрирует выраженные колебания,
		в то время как R1 и Optimistic приводят к стабилизации.
	}
	\end{figure}
	\paragraph{Вывод.}  
	Даже на сильно уменьшенном MNIST методы стабилизации (R1, instance noise, TTUR, Optimistic) показывают различия в динамике:  
	- Vanilla демонстрирует колебания и нестабильность.  
	- R1 и Instance noise уменьшают амплитуду колебаний.  
	- Optimistic GD создают более стабильное и демпфированное обучение с лучшей сходимостью.  
	\section{Абляции и анализ}
	
	В данном разделе мы исследуем влияние гиперпараметров стабилизации на
	динамическое поведение системы (спектр Якобиана, величина вращения,
	степень диссипации), а также сопоставляем эти признаки с качеством
	генерируемых образцов и метриками сходимости.
	
	\subsection{Влияние гиперпараметров на спектр и устойчивость}
	
	Мы проводим абляции по следующим параметрам:
	\begin{itemize}
		\item сила регуляризации $R_1$ (коэффициент $\lambda$);
		\item дисперсия \emph{instance noise} $\sigma^2$;
		\item отношение шагов TTUR, т.е.\ 
		\(
		\eta_D : \eta_G;
		\)
		\item размер мини-батча $B$.
	\end{itemize}
	
	Для каждого набора параметров мы вычисляем спектр Якобиана
	\[
	J = \frac{\partial v(\theta)}{\partial \theta},
	\qquad 
	v(\theta) =
	\begin{bmatrix}
		-\nabla_\phi L_G(\theta,\phi) \\[3pt]
		+\nabla_\theta L_D(\theta,\phi)
	\end{bmatrix},
	\]
	и анализируем его симметричную и кососимметричную части:
	\[
	J = S + A,
	\qquad
	S = \tfrac{1}{2}(J + J^\top),
	\quad
	A = \tfrac{1}{2}(J - J^\top).
	\]
	Симметрическая часть $S$ определяет локальную диссипацию/растяжение,
	кососимметричная часть $A$ определяет величину вращения.
	
	Мы фиксируем следующие диагностические показатели:
	\[
	\rho_{\mathrm{Re}} = \max_i \Re(\lambda_i(J)),
	\qquad
	\rho_{\mathrm{Im}} = \max_i |\Im(\lambda_i(J))|,
	\]
	а также нормы
	\[
	\|S\|_2, \qquad \|A\|_2,
	\]
	которые характеризуют силу диссипации и силу вращения соответственно.
	
	\paragraph{Регуляризация $R_1$.}
	
	При увеличении коэффициента $\lambda$:
	\begin{itemize}
		\item симметрическая часть $S$ становится более отрицательной,
		т.е.\ усиливается диссипация;
		\item $\rho_{\mathrm{Re}}$ уменьшается, что свидетельствует о подавлении
		локальной нестабильности;
		\item $\rho_{\mathrm{Im}}$ также уменьшается, что соответствует 
		подавлению вращения;
		\item при слишком большом $\lambda$ наблюдаем численную
		нестабильность (жёсткость динамики).
	\end{itemize}
	
	\paragraph{Instance noise.}
	
	Аддитивный шум к входам дискриминатора действует как сглаживание
	поля градиентов:
	\begin{itemize}
		\item уменьшает норму $\|A\|_2$, т.е.\ уменьшает антисимметричную часть;
		\item снижает мнимую часть спектра $\rho_{\mathrm{Im}}$;
		\item уменьшает амплитуду циклов в траекториях параметров.
	\end{itemize}
	
	\paragraph{TTUR.}
	
	При увеличении отношения шагов $\eta_D : \eta_G$:
	\begin{itemize}
		\item мнимая часть спектра $\rho_{\mathrm{Im}}$ уменьшается,
		т.к.\ дисбаланс шагов нарушает «чистую» ротацию;
		\item симметрическая часть $S$ становится доминирующей, что приводит
		к частичной стабилизации;
		\item при чрезмерном увеличении шага дискриминатора появляются
		колебания высокой частоты и ухудшение диссипации.
	\end{itemize}
	
	\paragraph{Размер батча.}
	
	Рост $B$ уменьшает вариативность оценки градиента, что приводит к:
	\begin{itemize}
		\item уменьшению размаха собственных значений;
		\item более регулярному поведению антисимметричной части $A$;
		\item снижению вероятности перехода собственных чисел на правую
		полуплоскость;
		\item уменьшению «дрожания» траекторий и частоты самопересечений.
	\end{itemize}
	
	
	\subsection{Связь динамических признаков со сходимостью и качеством образцов}
	
	Мы сопоставляем динамические показатели
	$\rho_{\mathrm{Re}}, \rho_{\mathrm{Im}}, \|S\|_2, \|A\|_2$
	с практическими метриками:
	стабильность обучения (вариация лоссов), скорость сходимости и качество
	сэмплов.
	
	Полученные зависимости обобщаются следующим образом.
	
	\begin{itemize}
		\item \textbf{Большая антисимметричная часть} $A$ 
		($\|A\|_2$ и $\rho_{\mathrm{Im}}$ велики):
		приводит к выраженным вращениям, циклам, спиралям и медленной
		сходимости; при этом качество сэмплов обычно низкое из-за отсутствия
		стационарного режима обучения.
		
		\item \textbf{Сильная отрицательная симметричная часть} $S$
		(диссипация):
		приводит к устойчивому поведению, монотонному уменьшению лоссов,
		стабилизации и улучшению качества образцов.
		Однако слишком большая диссипация может приводить к затуханию
		градиентов и медленной сходимости.
		
		\item \textbf{Методы EG/OGDA}:
		демонстрируют наименьшую $\rho_{\mathrm{Im}}$ и устойчивую отрицательную
		$\rho_{\mathrm{Re}}$, что коррелирует с наилучшей сходимостью и
		качеством образцов в toy-примере.
		
		\item \textbf{Instance noise и TTUR}:
		дают умеренное уменьшение вращения и частичное улучшение качества.
	\end{itemize}
	
	Таким образом, динамические характеристики спектра Якобиана являются
	предикторами качества сходимости и качества финального решения,
	и могут использоваться как инструмент анализа поведения методов в
	состязательном обучении.
	\section{Репозиторий и воспроизводимость}	
	Репозиторий содержит:
	\begin{itemize}
		\item README с инструкциями по установке и запуску;
		\item файл \texttt{requirements.txt} с зависимостями;
		\item фиксированные значения случайных seed;
		\item отдельные скрипты для экспериментов на 2D toy-GAN и MNIST;
		\item скрипты построения графиков (траектории параметров,
		динамика лоссов, эволюция спектра Якобиана).
	\end{itemize}
	
	Это позволяет полностью воспроизвести все представленные
	в работе результаты.
	

	\section{Литература}
	
	[1] Mescheder et al., NIPS 2018.  
	[2] Nagarajan, Kolter, 2017.  
	[3] Heusel et al., TTUR, 2017.  
	[4] Gulrajani et al., WGAN-GP, 2017.  
	[6] Daskalakis et al., Optimism, 2018.  
	[7] Gidel et al., VI perspective, 2019.  
	[8] Goodfellow et al., GAN, 2014.  
	
\end{document}